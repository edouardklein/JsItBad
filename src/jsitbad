#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
jsitbad: try to classify .js files as malicious or legitimate

Usage:
jsitbad add [-f] [-m|-l|-u] <database> <js-file> [<message>]
jsitbad fit <database>

Options:
  -m  Malicious
  -l  Legitimate
  -u  Unknown (default)
"""
from docopt import docopt
import logging
import os
import hashlib
import glob
import getpass
import socket
import datetime
import re
import requests
from slimit.lexer import Lexer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn import manifold
from sklearn import cross_validation
from sklearn import metrics
from sklearn.svm import LinearSVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
import numpy as np

# FIXME: Changer les print() en logging.* 

class Trainer():
    """Create and save the information about training a classifier on a Daabase"""
    def __init__(self, db, load=None):
        """Load or create the structure storing the training information"""
        self.db = db
        if not load:
            self.path = db.path+'/'+datetime.datetime.now().strftime('%Y%m%d%H%M%S')+'_training'
            if not os.path.isdir(self.path):
                os.makedirs(self.path)
        self.corpus = self.db.legitimate_snippets() + self.db.malicious_snippets()

    def write_pickle(self, var, name):
        """Serialize an object in our save dir"""
        with open(self.path+'/'+name+'.pickle', 'wb') as f:
            pickle.dump(var, f)

    def lex_corpus(self):
        '''Return our corpus as a list of text describing the JS tokens'''
        tokens_corpus = []
        for t in self.corpus:
            logging.debug('Lexing '+t.filename)
            try:
                lexer = Lexer()
                lexer.input(t.data)
                tokens_corpus.append(' '.join([token.type for token in lexer]))
            except TypeError as e:
                logging.error('Error lexing '+t.filename+' : '+str(e))
        return tokens_corpus

    def train_tfidf(self):
        '''Return the tf_idf transformer trained on our corpus'''
        count_vect = CountVectorizer()
        train_counts = count_vect.fit_transform(self.lex_corpus())
        tfidf_transformer = TfidfTransformer().fit(train_counts)
        self.write_pickle(tfidf_transformer, 'tfidf_transformer')
        def text2tfidf(text_list):
            '''Transform a list of text into a tfidf matrix'''
            return tfidf_transformer.transform(count_vect.transform(text_list)).toarray()
        return text2tfidf

    def clf_eval(self, clf, X, Y):
        """Train and evaluate a classifier"""
        loo = cross_validation.LeaveOneOut(len(Y))
        Y_true = []
        Y_pred = []
        for train_index, test_index in loo:
            X_train, X_test = X[train_index], X[test_index]
            Y_train, Y_test = Y[train_index], Y[test_index]
            clf.fit(X_train, Y_train)
            Y_pred.append(clf.predict(X_test))
            Y_true.append(Y_test)
        Y_true = np.array(Y_true)
        Y_pred = np.array(Y_pred)
        cm =  metrics.confusion_matrix(Y_true, Y_pred)
        logging.info("Overall accuracy : "+str(sum(Y_true == Y_pred)/len(Y_true)))
        return cm

    def single_projection(self,X, s, l, color, labels=None):
        Y = l.fit_transform(X)
        plt.title(s)
        plt.scatter(Y[:, 0], Y[:, 1], c=color, alpha=0.7)
        plt.axis('tight')
        if labels:
            place_labels(labels, Y)

    def project_on_plane(self, X, color, n_neighbors = 10, n_components = 2, title='2D projection', unique=None, labels=None):
        '''Give multiple 2D representations of a high-dimenstional dataset

    See http://scikit-learn.org/stable/auto_examples/manifold/plot_compare_methods.html'''
        fig = plt.figure(figsize=(15,8))
        learners = [['Isomap', manifold.Isomap(n_neighbors, n_components)],
                    ['LLE', manifold.LocallyLinearEmbedding(n_neighbors, n_components, eigen_solver='auto')],
                    ['LTSA', manifold.LocallyLinearEmbedding(n_neighbors, n_components, eigen_solver='auto', method='ltsa')],
                    ['Hessian', manifold.LocallyLinearEmbedding(n_neighbors, n_components, eigen_solver='auto', method='hessian')],
                    ["Modified", manifold.LocallyLinearEmbedding(n_neighbors, n_components, eigen_solver='auto', method='hessian')],
                    ["MDS", manifold.MDS(n_components, max_iter=100, n_init=1)],
                    ["Spectral Embedding", manifold.SpectralEmbedding(n_components=n_components,n_neighbors=n_neighbors)],
                    ["t-SNE", manifold.TSNE(n_components=n_components, init='pca', random_state=0)]
                ]
        fig_num = 331
        if unique:
            s,l = [x for x in learners if x[0] == unique][0]
            single_projection(X, s, l, color, labels)
        else:
            for s, l in learners:
                ax = fig.add_subplot(fig_num)
                fig_num += 1
                self.single_projection(X, s, l, color, labels)
        plt.savefig(self.path+'/'+title+'.pdf')

    def plot_results(self):
        """Plot the classifiers training information"""
        plt.figure(figsize=(10,10))
        for clf_str, (fp, fn) in self.results.items():
            plt.scatter(fp, fn)
            plt.text(fp, fn, clf_str)
        plt.title("Performance of different classification methods over the current database")
        plt.xlabel("# of False Positives")
        plt.ylabel("# of False Negatives")
        plt.savefig(self.path+'/FPFN.png')
        self.project_on_plane(self.X, ['r' if c == 1 else 'g' for c in self.Y])

    def fit(self):
        """Train multiple classifiers, print and save performance information"""
        self.tfidf = self.train_tfidf()
        X_0 = self.tfidf([s.data for s in self.db.legitimate_snippets()])
        X_1 = self.tfidf([s.data for s in self.db.malicious_snippets()])
        self.X = np.vstack([X_0, X_1])
        self.Y =np.zeros(len(self.X))
        self.Y[-len(X_1):] = 1
        self.write_pickle(self.X, 'X')
        self.write_pickle(self.Y, 'Y')
        self.results = {'Yes': [len(X_0), 0], 'No': [0, len(X_1)]}
        for str_clf in ['LinearSVC', 'KNeighborsClassifier', 'SVC', 'BaggingClassifier', 'RandomForestClassifier',
                        'ExtraTreesClassifier', 'AdaBoostClassifier', 'GradientBoostingClassifier']:
            clf = eval(str_clf+'()')
            logging.info("Evaluating "+str_clf+"...")
            cm = self.clf_eval(clf, self.X, self.Y)
            self.results[str_clf] = [cm[0,1], cm[1,0]]
            clf.fit(self.X,self.Y)
            self.write_pickle(clf, str_clf)
        self.write_pickle(self.results, "results")
        self.plot_results()


class Database():
    """A directory structure containing JavaScript samples and associated metadata"""
    def __init__(self, path):
        """Load or create the Database at path"""
        self.path = path
        self.subdirs = [self.path+'/'+sdir for sdir in ['malicious', 'legitimate', 'unknown']]
        if not self.check():
            self.create()

    def check(self):
        """Check if the database exists and contains the needed subdirs"""
        if not os.path.isdir(self.path):
            return False
        if not all(map(os.path.isdir, self.subdirs)):
            return False
        return True

    def create(self):
        """Create the directory structure needed to host a database"""
        for sdir in self.subdirs:
            os.makedirs(sdir)

    def lookup(self, sha1):
        """Return the snippet with the provided sha1, if it exists in the database"""
        logging.debug('Looking for snippet '+sha1)
        files=sum([glob.glob(sdir+'/*.js') for sdir in self.subdirs], [])
        matching_files = [f for f in files if os.path.basename(f)[:-3] == sha1]
        assert len(matching_files) <= 1, "Multiple snippets of the same hash in the DB"
        if matching_files:
            fname = matching_files[0]
            logging.debug('Found')
            return Snippet(filename=matching_files[0])
        logging.debug('Not found')
        return None

    def add_new(self, snippet, message):
        """Add to the database a snippet that wasn't there before, return the newly created snippet"""
        fname = self.path+'/'+snippet.status+'/'+snippet.sha1+'.js'
        with open(fname, 'wb') as f:
            f.write(snippet.data)
        snippet.filename = fname
        snippet.log(event='created', message=message)
        return snippet

    def move(self, snippet, status, message):
        """Moves an existing snippet from its status to another"""
        path = snippet.filename.split('/')
        new_name = '/'.join(path[:-2]+[status, path[-1]])
        os.rename(snippet.filename, new_name)
        #Now moving the logs
        os.rename(snippet.filename[:-2]+'log', new_name[:-2]+'log')
        snippet.filename = new_name
        old_status = snippet.status
        snippet.status = status
        snippet.log(event='moved from '+old_status, message=message)

    def legitimate_snippets(self):
        """The legitimate snippets in the DB"""
        return [Snippet(filename=fname) for fname in sorted(glob.glob(self.path+'/legitimate/*.js'))]

    def malicious_snippets(self):
        """The malicious snippets in the DB"""
        return [Snippet(filename=fname) for fname in sorted(glob.glob(self.path+'/malicious/*.js'))]


class Snippet():
    """A JavaScript snippet"""

    log_template = "{date} {severity} hostname={hostname} user={user} sha1={sha1} status={status} event={event} message={message}\n"

    def __init__(self, filename=None, status='unknown', sha1=None, data=None):
        """Create a new snippet instance, either from data and status, or from a file in a DB"""
        self.data = data
        if filename:
            self.loadFromFile(filename)
            return
        if not sha1:
            self.sha1 = hashlib.sha1(self.data).hexdigest()
        else:
            self.sha1 = sha1
        self.status = status

    def loadFromFile(self, fname):
        """Load a snippet from a file in a DB"""
        self.filename = fname
        self.__init__(data=open(fname, 'r').read(), status=fname.split('/')[-2], sha1=fname.split('/')[-1][:-3])

    def log(self, event, message):
        """Append a line to the snippet's log file"""
        if not self.filename:
            raise ValueError("We don't know which DB we belong to.")
        severity='ERROR:' if 'bad move' in event else 'WARNING:' if 'moved' in event else 'INFO:'
        log_message = self.log_template.format(date=datetime.datetime.now().isoformat(),
                                               severity=severity,
                                               hostname=socket.gethostname(),
                                               user=getpass.getuser(),
                                               sha1=self.sha1,
                                               status=self.status,
                                               event=event,
                                               message=message if message else '')
        log_fname = self.filename[:-3]+'.log'
        with open(log_fname, 'a') as f:
            f.write(log_message)


def do_add():
    """Add or move a snippet in the database"""
    database = Database(arguments['<database>'])
    status = 'malicious' if arguments['-m'] else 'legitimate' if arguments['-l'] else 'unknown'
    fname = arguments['<js-file>'] if arguments['<js-file>'] != '-' else '/dev/stdin'
    #REgexp from https://gist.github.com/gruber/8891611
    url_regexp = re.compile(r"""(?i)\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\s()<>{}\[\]]+|\([^\s()]*?\([^\s()]+\)[^\s()]*?\)|\([^\s]+?\))+(?:\([^\s()]*?\([^\s()]+\)[^\s()]*?\)|\([^\s]+?\)|[^\s`!()\[\]{};:'".,<>?«»“”‘’])|(?:(?<!@)[a-z0-9]+(?:[.\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\b/?(?!@)))""")
    if url_regexp.match(fname):
        # I wouldn't need to lie if people did not forbid robot to fetch files.
        headers = {'User-Agent': "Mozilla/5.0 (X11; U; Linux i686) Gecko/20071127 Firefox/2.0.0.11"}
        data = requests.get(fname, headers=headers).content
    else:
        print("DEBUG: {} does not match".format(fname))
        with open(fname, 'rb') as f:
            data = f.read()
    snippet = Snippet(data=data, status=status)
    existing = database.lookup(snippet.sha1)
    if not existing:
        snippet = database.add_new(snippet, arguments['<message>'])
    elif existing.status == snippet.status:
        existing.log(event='seen', message=arguments['<message>'])
    elif existing.status == 'unknown' or arguments['-f']:
        if not arguments['<message>']:
            existing.log(event='bad move to '+snippet.status, message='reason="no message""' )
            raise RuntimeError("Please provide a message when changing a snippet's status")
        snippet = database.move(existing, snippet.status, arguments['<message>'])
    else:
        existing.log(event='bad move to '+snippet.status,
                     message='reason="no -f option" '+str(arguments['<message>']))
        raise RuntimeError("Please use the -f option if you want to change the status of a malicious"
        "or legitimate sample")


def do_fit():
    """Train a classifier on a database"""
    database = Database(arguments['<database>'])
    trainer = Trainer(database)
    trainer.fit()
    
if __name__ == '__main__':
    arguments = docopt(__doc__, version='JsItBad 0.0')
    logging.basicConfig(level=logging.DEBUG)
    if arguments['add']:
        do_add()
    elif arguments['fit']:
        do_fit()
